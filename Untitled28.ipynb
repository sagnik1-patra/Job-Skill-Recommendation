{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bf52678-5968-45cd-bdd6-17e55da0c0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "Prediction Result:\n",
      "{\n",
      "  \"timestamp\": \"2025-09-21 22:12:34\",\n",
      "  \"user_current_skills\": [\n",
      "    \"communication\",\n",
      "    \"recruitment\",\n",
      "    \"employee relations\"\n",
      "  ],\n",
      "  \"target_job_title\": \"HR Manager\",\n",
      "  \"target_industry\": \"Human Resources\",\n",
      "  \"recommended_skills\": [\n",
      "    \"'communication'\",\n",
      "    \"'problem solving'\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "Saved (appended) to:\n",
      " - CSV: C:\\Users\\NXTWAVE\\Downloads\\Job Skill Recommendation\\skill_recommendation_history.csv\n",
      " - JSON: C:\\Users\\NXTWAVE\\Downloads\\Job Skill Recommendation\\skill_recommendation_history.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# -----------------------------\n",
    "# Config / Paths\n",
    "# -----------------------------\n",
    "OUTPUT_DIR = r\"C:\\Users\\NXTWAVE\\Downloads\\Job Skill Recommendation\"\n",
    "TFIDF_PATH = os.path.join(OUTPUT_DIR, \"tfidf_vectorizer.pkl\")\n",
    "MLB_PATH = os.path.join(OUTPUT_DIR, \"mlb.pkl\")\n",
    "MODEL_PATH = os.path.join(OUTPUT_DIR, \"skill_model.keras\")\n",
    "\n",
    "CSV_LOG_PATH = os.path.join(OUTPUT_DIR, \"skill_recommendation_history.csv\")\n",
    "JSON_LOG_PATH = os.path.join(OUTPUT_DIR, \"skill_recommendation_history.json\")\n",
    "\n",
    "# Create output dir just in case\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Load artifacts\n",
    "# -----------------------------\n",
    "if not os.path.exists(TFIDF_PATH) or not os.path.exists(MLB_PATH) or not os.path.exists(MODEL_PATH):\n",
    "    raise FileNotFoundError(\n",
    "        \"One or more model artifacts are missing. Make sure tfidf_vectorizer.pkl, mlb.pkl and skill_model.keras exist in:\\n\"\n",
    "        f\"{OUTPUT_DIR}\"\n",
    "    )\n",
    "\n",
    "with open(TFIDF_PATH, \"rb\") as f:\n",
    "    tfidf = pickle.load(f)\n",
    "\n",
    "with open(MLB_PATH, \"rb\") as f:\n",
    "    mlb = pickle.load(f)\n",
    "\n",
    "model = load_model(MODEL_PATH)\n",
    "\n",
    "# -----------------------------\n",
    "# Preprocessing (same as training)\n",
    "# -----------------------------\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Lowercase, remove non-alphanumerics, tokenize, remove stopwords, lemmatize,\n",
    "    return joined string.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        text = \"\"\n",
    "    # ensure string\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# -----------------------------\n",
    "# Recommendation function\n",
    "# -----------------------------\n",
    "def recommend_skills(user_skills, job_title=\"\", industry=\"\", top_n=10, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Returns a list of top_n recommended skills (predicted by the trained model).\n",
    "    The model is multi-label; we threshold probabilities and use mlb.inverse_transform.\n",
    "    If many predicted skills, we take first top_n.\n",
    "    \"\"\"\n",
    "    # build combined user text\n",
    "    user_text = \"\"\n",
    "    if isinstance(user_skills, (list, tuple)):\n",
    "        user_text += \" \" + \" \".join(user_skills)\n",
    "    else:\n",
    "        user_text += \" \" + str(user_skills)\n",
    "\n",
    "    if job_title:\n",
    "        user_text += \" \" + str(job_title)\n",
    "    if industry:\n",
    "        user_text += \" \" + str(industry)\n",
    "\n",
    "    # preprocess and vectorize\n",
    "    X_user = tfidf.transform([preprocess_text(user_text)])\n",
    "    # predict probabilities\n",
    "    y_prob = model.predict(X_user.toarray())\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "\n",
    "    # get predicted skill tuples from mlb\n",
    "    predicted_skill_tuples = mlb.inverse_transform(y_pred)\n",
    "    # flatten (mlb.inverse_transform returns list of tuples)\n",
    "    predicted_flat = []\n",
    "    for tup in predicted_skill_tuples:\n",
    "        predicted_flat.extend(list(tup))\n",
    "\n",
    "    # deduplicate while preserving order\n",
    "    seen = set()\n",
    "    deduped = []\n",
    "    for sk in predicted_flat:\n",
    "        if sk not in seen:\n",
    "            deduped.append(sk)\n",
    "            seen.add(sk)\n",
    "\n",
    "    # If model predicted fewer than top_n, fall back to content-based nearest jobs (optional)\n",
    "    # but for now just return first top_n\n",
    "    return deduped[:top_n]\n",
    "\n",
    "# -----------------------------\n",
    "# Append result to CSV\n",
    "# -----------------------------\n",
    "def append_to_csv(row_dict, csv_path=CSV_LOG_PATH):\n",
    "    \"\"\"\n",
    "    Appends a single-row dict to CSV. If CSV missing, creates it.\n",
    "    Lists are stored as JSON strings in the CSV for readability.\n",
    "    \"\"\"\n",
    "    df_row = pd.DataFrame([{\n",
    "        k: (json.dumps(v, ensure_ascii=False) if isinstance(v, (list, dict)) else v)\n",
    "        for k, v in row_dict.items()\n",
    "    }])\n",
    "    if not os.path.exists(csv_path):\n",
    "        df_row.to_csv(csv_path, index=False)\n",
    "    else:\n",
    "        df_row.to_csv(csv_path, index=False, header=False, mode='a')\n",
    "\n",
    "# -----------------------------\n",
    "# Append result to JSON array file\n",
    "# -----------------------------\n",
    "def append_to_json(row_dict, json_path=JSON_LOG_PATH):\n",
    "    \"\"\"\n",
    "    Maintains a JSON array in a file. If file not present, creates a new array.\n",
    "    Appends the new dict and writes back (reads whole file).\n",
    "    \"\"\"\n",
    "    # Load existing list if exists\n",
    "    data = []\n",
    "    if os.path.exists(json_path):\n",
    "        try:\n",
    "            with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "                if not isinstance(data, list):\n",
    "                    # if file corrupted or not a list, start fresh\n",
    "                    data = []\n",
    "        except Exception:\n",
    "            # any error reading -> start fresh\n",
    "            data = []\n",
    "\n",
    "    # append new\n",
    "    data.append(row_dict)\n",
    "\n",
    "    # write back\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# -----------------------------\n",
    "# High-level function: predict + append\n",
    "# -----------------------------\n",
    "def predict_and_save(user_skills, job_title=\"\", industry=\"\", top_n=10, save_csv=True, save_json=True):\n",
    "    recommended = recommend_skills(user_skills, job_title, industry, top_n=top_n)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    row = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"user_current_skills\": user_skills,\n",
    "        \"target_job_title\": job_title,\n",
    "        \"target_industry\": industry,\n",
    "        \"recommended_skills\": recommended\n",
    "    }\n",
    "\n",
    "    if save_csv:\n",
    "        append_to_csv(row, CSV_LOG_PATH)\n",
    "    if save_json:\n",
    "        append_to_json(row, JSON_LOG_PATH)\n",
    "\n",
    "    return row\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage (HR role)\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Example HR input\n",
    "    user_skills_input = [\"communication\", \"recruitment\", \"employee relations\"]\n",
    "    job_title_input = \"HR Manager\"\n",
    "    industry_input = \"Human Resources\"\n",
    "\n",
    "    result = predict_and_save(\n",
    "        user_skills=user_skills_input,\n",
    "        job_title=job_title_input,\n",
    "        industry=industry_input,\n",
    "        top_n=10\n",
    "    )\n",
    "\n",
    "    print(\"Prediction Result:\")\n",
    "    print(json.dumps(result, indent=2, ensure_ascii=False))\n",
    "    print(f\"\\nSaved (appended) to:\\n - CSV: {CSV_LOG_PATH}\\n - JSON: {JSON_LOG_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f774412-99b6-40db-8d41-4c247aeaae74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
